optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.Adam
  #  These are all default parameters for the Adam optimizer
  lr: 0.00001
  betas: [ 0.9, 0.999 ]
  eps: 1e-08
  weight_decay: 0

use_lr_scheduler: False
lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 10
  T_mult: 2
  eta_min: 0 # min value for the lr
  last_epoch: -1
  verbose: True
